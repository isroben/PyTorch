{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a771e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch import no_grad\n",
    "from torch.nn import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2433c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3d7c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256), # Resize shorter edge to 256\n",
    "    transforms.CenterCrop(224), # Extract 224*224 center square\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b3434b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordFlowersDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load Matlab labels\n",
    "        labels_mat = scipy.io.loadmat(os.path.join(root_dir, 'imagelabels.mat'))\n",
    "        self.labels = labels_mat['labels'][0] - 1\n",
    "\n",
    "        # Keep track of any errors we encounter\n",
    "        self.error_log = []\n",
    "\n",
    "        print(len(self.labels)) # Number of image labels\n",
    "\n",
    "        # MIN Label\n",
    "        print(f\"Min Label: {self.labels.min()}\")\n",
    "        # MAX Label\n",
    "        print(f\"Min Label: {self.labels.max()}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 8189 samples\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "        # Build the image filename\n",
    "            img_name = f'image_{idx+1:05d}.jpg'\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "\n",
    "            # Check for corruption\n",
    "            image.verify() # Verify() closes the file\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            # Skip tiny images\n",
    "            if image.size[0] < 32 or image.size[1] < 32:\n",
    "                raise ValueError(f\"Image too small: {image.size}\")\n",
    "            \n",
    "            # Covert grayscale to RGB\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "\n",
    "            # Load the image\n",
    "            image = Image.open(img_path)\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            # Log the issue instead of crashing\n",
    "            self.error_log.append({\n",
    "                'index': idx,\n",
    "                'error': str(e),\n",
    "                'path': img_path if 'img_path' in locals() else 'unknown'\n",
    "            })\n",
    "\n",
    "            print(f\"Warning: Skipping corrupted image {idx}: {e}\")\n",
    "            # Try the next image (warp around if needed)\n",
    "            next_idx = (idx + 1) % len(self)\n",
    "            return self.__getitem__(next_idx)\n",
    "        \n",
    "    def get_error_summary(self):\n",
    "        \"\"\"Review what went wrong after training.\"\"\"\n",
    "        if not self.error_log:\n",
    "            print(\"No errors encountered - dataset is clean\")\n",
    "        else:\n",
    "            print(f\"\\nEncountered {len(self.error_log)} Problematic images: \")\n",
    "            for error in self.error_log[:5]: # Show first 5\n",
    "                print(f\"Index {error['index']}: {error['error']}\")\n",
    "                if len(self.error_log):\n",
    "                    print(f\" ... and {len(self.error_log) - 5} more\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8189\n",
      "Min Label: 0\n",
      "Min Label: 101\n",
      "Total samples: 8189\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "dataset = OxfordFlowersDataset(\n",
    "    r\"Oxford_102_flowers\", transform=transform\n",
    "    )\n",
    "print(f\"Total samples: {len(dataset)}\") # Shows 8189\n",
    "# Try loading one image\n",
    "img, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474926f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc306de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: <built-in method size of Tensor object at 0x00000287544ACE30>\n",
      "Image 0: <class 'torch.Tensor'>\n",
      "Image 100: <built-in method size of Tensor object at 0x0000028754522570>\n",
      "Image 100: <class 'torch.Tensor'>\n",
      "Image 500: <built-in method size of Tensor object at 0x00000287532D2090>\n",
      "Image 500: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Check a few images\n",
    "for i in [0, 100, 500]:\n",
    "    img, _ = dataset[i]\n",
    "    print(f'Image {i}: {img.size}')\n",
    "    print(f'Image {i}: {type(img)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1569f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Resize((224, 224)) # Can cause size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66b770e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, _ = dataset[0]\n",
    "# resized = transforms.Resize(256)(img)\n",
    "# print(f\"After resize: {resized.size}\")  # (302,256) - Keeps aspect ratio!\n",
    "# cropped = transforms.CenterCrop(224)(resized)\n",
    "# print(f\"After crop: {cropped.size}\")    # (224,224) - Perfect Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00bfd580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = dataset[1]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83855a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a8808d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test: 70/15/15\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * (len(dataset)))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51384bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 5732\n",
      "Training: 1228\n",
      "Training: 1229\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training: {len(train_dataset)}\")\n",
    "print(f\"Training: {len(val_dataset)}\")\n",
    "print(f\"Training: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16f588bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d042136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([32, 3, 224, 224])\n",
      "Batch of labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74a89881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 178: 32 images\n",
      "Batch 179: 32 images\n",
      "Batch 180: 4 images\n",
      "\n",
      "Total batches in one epoch: 180\n",
      "total_images seen: 5732\n"
     ]
    }
   ],
   "source": [
    "# Batching Numbers\n",
    "batch_count = 0\n",
    "total_images = 0\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    batch_count += 1\n",
    "    total_images += len(images)\n",
    "\n",
    "    # Show the last few batches\n",
    "    if batch_count >= 178:\n",
    "        print(f\"Batch {batch_count}: {len(images)} images\")\n",
    "\n",
    "print(f\"\\nTotal batches in one epoch: {batch_count}\")\n",
    "print(f'total_images seen: {total_images}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca14b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 180 batches\n",
      "Val: 39 batches\n",
      "Train: 39 batches\n",
      "Train batch: torch.Size([32, 3, 224, 224])\n",
      "Val batch: torch.Size([32, 3, 224, 224])\n",
      "Test batch: torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Verify everything works\n",
    "print(f\"Train: {len(train_loader)} batches\")\n",
    "print(f\"Val: {len(val_loader)} batches\")\n",
    "print(f\"Train: {len(test_loader)} batches\")\n",
    "\n",
    "# Quick test - get one batch from each\n",
    "for name, loader, in [(\"Train\", train_loader), (\"Val\", val_loader), (\"Test\", test_loader)]:\n",
    "    images, labels = next(iter(loader))\n",
    "    print(f\"{name} batch: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8a992",
   "metadata": {},
   "source": [
    "# Augmentation Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0637fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Transformations - with random augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    # Random augmentations (different each time)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "\n",
    "    # Standard Preprocessing\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transformation\n",
    "val_transforms = transforms.Compose([\n",
    "    # Only Standard Preprocessing\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b77877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d40be808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_augmentations(dataset, idx=0, num_version=8):\n",
    "#     \"\"\" See what augmentation actually does to your images\"\"\"\n",
    "#     fig, axes = plt.subplot(2, 4, figsize=(12, 6))\n",
    "#     axes = axes.Flatten()\n",
    "\n",
    "#     for i in range(num_version):\n",
    "#         img, label = dataset[idx] # Get augmented version\n",
    "\n",
    "#         # Denormalize for display\n",
    "#         img = denormalize(img)\n",
    "\n",
    "#         axes[i].imshow(img.permute(1,2,0)) # CHW --> HWC\n",
    "#         axes[i].set_title(f\"version {i+1}\")\n",
    "#         axes[i].axis('Off')\n",
    "\n",
    "#         plt.subtitle(f\"Same flower (index {idx}), 8 different augmentations\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a4fc2",
   "metadata": {},
   "source": [
    "# Define a simple CNN with only conv2d layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a2d7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolution block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolution block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolution block\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Input image is 32x32, after 3 pooling layers : 4x4\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 102) # 15 classes in dataset\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        # print(f\"Shape of conv1: {x.shape}\")\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        # print(f\"Shape of conv2: {x.shape}\")\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Third conv block\n",
    "        x = self.conv3(x)\n",
    "        # print(f\"Shape of conv3: {x.shape}\")\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        # print(f\"Shape of pool3: {x.shape}\")\n",
    "\n",
    "\n",
    "        # Flatten before the fully connected layer\n",
    "        x = self.flatten(x)\n",
    "        # print(f\"Shape of Flatten: {x.shape}\")\n",
    "\n",
    "\n",
    "      # Fully connected layers\n",
    "        # Input image is 32x32, after 3 pooling layers : 4x4\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43bf3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=102, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of our CNN\n",
    "model = SimpleCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75909a46",
   "metadata": {},
   "source": [
    "## Defining loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf93e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09d7f6",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, loss_function, optimizer, device=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track progress\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        # Print every 100 batches\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            avg_loss = running_loss/100\n",
    "            accuracy = 100. * correct / total\n",
    "            print(f\" [{batch_idx * 64}/{60000}]\"\n",
    "                  f\"Loss: {avg_loss:.3f} | Accuracy: {accuracy:.1f}%\")\n",
    "            running_loss = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e9947",
   "metadata": {},
   "source": [
    "## Setting up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d207a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, test_loader, device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # inputs, targets = input.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _,predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d342c20",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bdb57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      " [6400/60000]Loss: 4.346 | Accuracy: 6.7%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     accuracy = evaluation(model, val_loader)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, loss_function, optimizer, device)\u001b[39m\n\u001b[32m     11\u001b[39m output = model(data)\n\u001b[32m     12\u001b[39m loss = loss_function(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m optimizer.step()\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Track progress\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roben\\Codes\\Deep_Learning\\tf_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roben\\Codes\\Deep_Learning\\tf_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\roben\\Codes\\Deep_Learning\\tf_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch: {epoch+1}\")\n",
    "    train_epoch(model, train_loader, loss_function, optimizer)\n",
    "    accuracy = evaluation(model, val_loader)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e0ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99671a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "MODEL_PATH = Path('Saved_Models')\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME = \"Oxford_Flower_Classification.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "MODEL_SAVE_PATH\n",
    "\n",
    "# Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84295ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a PyTorch Model\n",
    "\n",
    "# Create a new instance of linear regression model\n",
    "loaded_model = SimpleCNN()\n",
    "\n",
    "# Load the saved model state_dict\n",
    "loaded_model.state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Put the loaded model to device\n",
    "# loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(loaded_model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b8a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loaded model\n",
    "with no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # inputs, targets = input.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Example for classification\n",
    "    probabilities = torch.softmax(outputs, dim=)\n",
    "    predicted_class = torch.argmax(probabilities, dim=)\n",
    "    print(f\"Predicted class: {predicted_class.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31927e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x28857632d90>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151df4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
